{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ghmm\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import pysam\n",
    "import os\n",
    "import pandas\n",
    "import re\n",
    "import editdistance\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "%run \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/alignment_lib.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NMERS = 3\n",
    "NSTATES = 4**NMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HMM_PARAMS = pickle.load(open(\"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/hmm_params.pickle\", 'rb'))\n",
    "HMM_PARAMS = sorted(zip(*[HMM_PARAMS[\"kmers\"], HMM_PARAMS[\"means\"], HMM_PARAMS[\"stdv\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_transmat(nmers):\n",
    "    \"\"\"make a transition matrix assuming single base steps\"\"\"\n",
    "    all_kmers = [x[0] for x in HMM_PARAMS]\n",
    "    n_components = len(all_kmers)\n",
    "    transmat = np.empty((n_components, n_components))\n",
    "    for j, from_kmer in enumerate(all_kmers):\n",
    "        for i, to_kmer in enumerate(all_kmers):\n",
    "            p = 1/4. if from_kmer[-2:] == to_kmer[:2] else 0.\n",
    "            transmat[j, i] = p          \n",
    "            \n",
    "    return transmat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example code for a continuous HMM with gaussian emissions\n",
    "\n",
    "\n",
    "F = ghmm.Float()  # emission domain of this model\n",
    "\n",
    "A = mk_transmat(NMERS)\n",
    "B = [[x[1], x[2]] for x in HMM_PARAMS]   # parameters of emission distributions in pairs of (mu, sigma)\n",
    "pi = [1/float(NSTATES)] * NSTATES   # initial probabilities per state\n",
    "\n",
    "# generate model from parameters\n",
    "model = ghmm.HMMFromMatrices(F,ghmm.GaussianDistribution(F), A, B, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianEmissionHMM(N=64)\n",
      "  state 0 (initial=0.02, mu=70.17, sigma=3.84)\n",
      "    Transitions: ->0 (0.25), ->1 (0.25), ->2 (0.25), ->3 (0.25)\n",
      "  state 1 (initial=0.02, mu=63.14, sigma=3.47)\n",
      "    Transitions: ->4 (0.25), ->5 (0.25), ->6 (0.25), ->7 (0.25)\n",
      "\n",
      "  ...\n",
      "\n",
      "  state 62 (initial=0.02, mu=53.52, sigma=2.85)\n",
      "    Transitions: ->56 (0.25), ->57 (0.25), ->58 (0.25), ->59 (0.25)\n",
      "  state 63 (initial=0.02, mu=49.86, sigma=2.78)\n",
      "    Transitions: ->60 (0.25), ->61 (0.25), ->62 (0.25), ->63 (0.25)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = str(model)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def result_to_seq(result):\n",
    "    states = result[0]\n",
    "    all_kmers = [x[0] for x in HMM_PARAMS]\n",
    "    kmers = [all_kmers[x] for x in states]\n",
    "    seq = [kmer[NMERS/2] for kmer in kmers]\n",
    "    return \"\".join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(means):\n",
    "    seq = ghmm.EmissionSequence(F, means)\n",
    "    result = model.viterbi(seq)\n",
    "    return result_to_seq(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'TTTGTGTGT'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([44.2, 44.3, 56, 58.2, 56.2, 58.1, 58.2, 60, 30.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"events\" : \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/david_events.template.pickle\",\n",
    "    \"out_basename\" : \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/david_calling\",\n",
    "    \"ref\": \"/home/ibis/gregor.sturm/nanopore/david_eccles_bc_ideas/mouse_ref.fa\",\n",
    "    \"ncores\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert os.path.isfile(args[\"events\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>mmusMT_PCR1']\n",
      "GTTAATGTAGCTTAATAACAAAGCAAAGCACTGAAAATGCTTAGATGGATAATTGTATCCCATAAACACAAAGGTTTGGTCCTGGCCTTATAATTAATTA\n"
     ]
    }
   ],
   "source": [
    "ref_file = args[\"ref\"]\n",
    "test = !cat {ref_file} | grep \">\"\n",
    "print(test)\n",
    "ref = !cat {ref_file} | grep -v \">\"\n",
    "ref = ref[0]\n",
    "print(ref[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_data = pickle.load(open(args[\"events\"], 'rb'))\n",
    "file_data = [f for f in file_data if f is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepare_filemap(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fastq_file = \"{0}.fastq\".format(args[\"out_basename\"])\n",
    "mk_fastq(fastq_file, file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Index 12:19:11] Running in fast and sensitive mode. Two indexes will be used (double memory consumption).\n",
      "[Index 12:19:11] Index already exists. Loading from file.\n",
      "[Index 12:19:11] Secondary index already exists. Loading from file.\n",
      "[Index 12:19:11] Index loaded in 0.15 sec.\n",
      "[Index 12:19:11] Memory consumption: [currentRSS = 513 MB, peakRSS = 513 MB]\n",
      "\n",
      "[Run 12:19:11] Automatically setting the maximum allowed number of regions: max. 500, attempt to reduce after 100\n",
      "[Run 12:19:11] Reference genome is assumed to be linear.\n",
      "[Run 12:19:11] Only one alignment will be reported per mapped read.\n",
      "[ProcessReads 12:19:11] Reads will be loaded in batches of up to 200 MB in size.\n",
      "[ProcessReads 12:19:11] Batch of 81 reads (0 MiB) loaded in 0.00 sec. (16442488 bases)\n",
      "[ProcessReads 12:19:11] Memory consumption: [currentRSS = 514 MB, peakRSS = 514 MB]\n",
      "[ProcessReads 12:19:11] Using 4 threads.\n",
      "[ProcessReads 12:19:12] [CPU time: 3.40 sec, RSS: 537 MB] Read: 81/81 (100.00%) [m: 80, u: 1]                                                                      \n",
      "\n",
      "[ProcessReads 12:19:12] Memory consumption: [currentRSS = 537 MB, peakRSS = 582 MB]\n",
      "\n",
      "[ProcessReads 12:19:12] All reads processed in 3.40 sec (or 0.06 CPU min).\n"
     ]
    }
   ],
   "source": [
    "sam_file = \"{0}.sam\".format(args[\"out_basename\"])\n",
    "graphmap(ref_file, fastq_file, sam_file, args[\"ncores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[samopen] SAM header is present: 1 sequences.\r\n"
     ]
    }
   ],
   "source": [
    "bam_file = prepare_sam(args[\"out_basename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samfile = pysam.AlignmentFile(bam_file)\n",
    "reads  = [x for x in samfile.fetch()]\n",
    "len(reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basecall_read(params):\n",
    "    read_name, ref_pos = params\n",
    "    file_id, channel_id = get_file_and_channel(read_name)\n",
    "    print(file_id, channel_id, read_name)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    file_obj = get_file(channel_id, file_id)\n",
    "      \n",
    "    events = [x[\"mean\"] for x in file_obj[\"events\"].to_dict(\"records\")]\n",
    "    metrichor_seq = file_obj[\"fastq\"].split(\"\\n\")[1]\n",
    "    ref_seq = \"\".join([ref[x] for x in ref_pos])\n",
    "    called_seq = predict(events)\n",
    "    \n",
    "    stats = {\n",
    "        \"d_metrichor\": int(editdistance.eval(ref_seq, metrichor_seq)),\n",
    "        \"d_caller\": int(editdistance.eval(ref_seq, called_seq)), \n",
    "        \"length\": len(ref_seq),\n",
    "    }\n",
    "\n",
    "    return (read_name, called_seq, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 132, 'ch132_file10_read')\n",
      "(9, 215, 'ch215_file9_read')\n",
      "(17, 132, 'ch132_file17_read')\n",
      "(16, 215, 'ch215_file16_read')\n",
      "(35, 132, 'ch132_file35_read')\n",
      "(1, 203, 'ch203_file1_read')\n",
      "(20, 211, 'ch211_file20_read')\n",
      "(12, 206, 'ch206_file12_read')\n",
      "(11, 133, 'ch133_file11_read')\n",
      "(28, 211, 'ch211_file28_read')\n",
      "(25, 215, 'ch215_file25_read')\n",
      "(11, 156, 'ch156_file11_read')\n",
      "(1, 157, 'ch157_file1_read')\n",
      "(0, 135, 'ch135_file0_read')\n",
      "(5, 227, 'ch227_file5_read')\n",
      "(10, 156, 'ch156_file10_read')\n",
      "(12, 211, 'ch211_file12_read')\n",
      "(21, 212, 'ch212_file21_read')\n",
      "(3, 211, 'ch211_file3_read')\n",
      "(19, 135, 'ch135_file19_read')\n",
      "(12, 141, 'ch141_file12_read')\n",
      "(18, 142, 'ch142_file18_read')\n",
      "(14, 215, 'ch215_file14_read')\n",
      "(8, 141, 'ch141_file8_read')\n",
      "(3, 195, 'ch195_file3_read')\n",
      "(15, 215, 'ch215_file15_read')\n",
      "(13, 141, 'ch141_file13_read')\n",
      "(10, 215, 'ch215_file10_read')\n",
      "(24, 212, 'ch212_file24_read')\n",
      "(27, 212, 'ch212_file27_read')\n",
      "(6, 156, 'ch156_file6_read')\n",
      "(3, 156, 'ch156_file3_read')\n",
      "(0, 157, 'ch157_file0_read')\n",
      "(26, 132, 'ch132_file26_read')\n",
      "(2, 203, 'ch203_file2_read')\n",
      "(5, 204, 'ch204_file5_read')\n",
      "(17, 135, 'ch135_file17_read')\n",
      "(11, 223, 'ch223_file11_read')\n",
      "(38, 132, 'ch132_file38_read')\n",
      "(27, 215, 'ch215_file27_read')\n",
      "(6, 223, 'ch223_file6_read')\n",
      "(10, 206, 'ch206_file10_read')\n",
      "(14, 149, 'ch149_file14_read')\n",
      "(17, 215, 'ch215_file17_read')\n",
      "(44, 132, 'ch132_file44_read')\n",
      "(29, 211, 'ch211_file29_read')\n",
      "(7, 206, 'ch206_file7_read')\n",
      "(23, 156, 'ch156_file23_read')\n",
      "(8, 204, 'ch204_file8_read')\n",
      "(1, 201, 'ch201_file1_read')\n",
      "(14, 209, 'ch209_file14_read')\n",
      "(3, 142, 'ch142_file3_read')\n",
      "(21, 223, 'ch223_file21_read')\n",
      "(34, 206, 'ch206_file34_read')\n",
      "(32, 135, 'ch135_file32_read')\n",
      "(36, 132, 'ch132_file36_read')\n",
      "(17, 211, 'ch211_file17_read')\n",
      "(20, 206, 'ch206_file20_read')\n",
      "(33, 135, 'ch135_file33_read')\n",
      "(12, 149, 'ch149_file12_read')\n",
      "(35, 211, 'ch211_file35_read')\n",
      "(26, 211, 'ch211_file26_read')\n",
      "(23, 215, 'ch215_file23_read')\n",
      "(4, 204, 'ch204_file4_read')\n",
      "(15, 156, 'ch156_file15_read')\n",
      "(25, 132, 'ch132_file25_read')\n",
      "(3, 149, 'ch149_file3_read')\n",
      "(25, 135, 'ch135_file25_read')\n",
      "(23, 135, 'ch135_file23_read')\n",
      "(12, 215, 'ch215_file12_read')\n",
      "(15, 141, 'ch141_file15_read')\n",
      "(2, 133, 'ch133_file2_read')\n",
      "(21, 206, 'ch206_file21_read')\n",
      "(23, 132, 'ch132_file23_read')\n",
      "(3, 132, 'ch132_file3_read')\n",
      "(4, 223, 'ch223_file4_read')\n",
      "(4, 206, 'ch206_file4_read')\n",
      "(4, 215, 'ch215_file4_read')\n",
      "(9, 206, 'ch206_file9_read')\n",
      "(3, 213, 'ch213_file3_read')\n"
     ]
    }
   ],
   "source": [
    "p = Pool(args[\"ncores\"])\n",
    "## one can not access a read in parallel (deadlock for whatever reason)\n",
    "## therfore prepare input parameters outside of map\n",
    "input_params = [(read.query_name, read.get_reference_positions()) for read in reads]\n",
    "try:\n",
    "    results = p.map(basecall_read, input_params)\n",
    "except KeyboardInterrupt:\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers, seqs, stats = zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats = pandas.DataFrame(list(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_caller       226446\n",
       "d_metrichor    172867\n",
       "length         387166\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fasta_file_called = \"{0}.called.fa\".format(args[\"out_basename\"])\n",
    "with open(fasta_file_called, 'w') as f: \n",
    "    for header, seq in zip(headers, seqs): \n",
    "        f.write(\">\" + header + \"\\n\")\n",
    "        f.write(seq + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Index 12:20:19] Running in fast and sensitive mode. Two indexes will be used (double memory consumption).\n",
      "[Index 12:20:19] Index already exists. Loading from file.\n",
      "[Index 12:20:19] Secondary index already exists. Loading from file.\n",
      "[Index 12:20:19] Index loaded in 0.15 sec.\n",
      "[Index 12:20:19] Memory consumption: [currentRSS = 513 MB, peakRSS = 513 MB]\n",
      "\n",
      "[Run 12:20:19] Automatically setting the maximum allowed number of regions: max. 500, attempt to reduce after 100\n",
      "[Run 12:20:19] Reference genome is assumed to be linear.\n",
      "[Run 12:20:19] Only one alignment will be reported per mapped read.\n",
      "[ProcessReads 12:20:19] Reads will be loaded in batches of up to 200 MB in size.\n",
      "[ProcessReads 12:20:19] Batch of 80 reads (0 MiB) loaded in 0.00 sec. (31380616 bases)\n",
      "[ProcessReads 12:20:19] Memory consumption: [currentRSS = 514 MB, peakRSS = 514 MB]\n",
      "[ProcessReads 12:20:19] Using 4 threads.\n",
      "[ProcessReads 12:20:20] [CPU time: 3.40 sec, RSS: 529 MB] Read: 80/80 (100.00%) [m: 79, u: 1]                                                                      \n",
      "\n",
      "[ProcessReads 12:20:20] Memory consumption: [currentRSS = 529 MB, peakRSS = 574 MB]\n",
      "\n",
      "[ProcessReads 12:20:20] All reads processed in 3.40 sec (or 0.06 CPU min).\n"
     ]
    }
   ],
   "source": [
    "sam_file_called = \"{0}.called.sam\".format(args[\"out_basename\"])\n",
    "graphmap(ref_file, fasta_file_called, sam_file_called, args[\"ncores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[samopen] SAM header is present: 1 sequences.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/david_calling.called.sorted.bam'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sam(\"{0}.called\".format(args[\"out_basename\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 2)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
