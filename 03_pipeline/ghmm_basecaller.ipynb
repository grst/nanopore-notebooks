{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ghmm\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import pysam\n",
    "import os\n",
    "import pandas\n",
    "import re\n",
    "import editdistance\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "%run \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/alignment_lib.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NMERS = 3\n",
    "NSTATES = 4**NMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"events\" : \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/david_events.template.pickle\",\n",
    "    \"out_basename\" : \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/david_calling\",\n",
    "    \"ref\": \"/home/ibis/gregor.sturm/nanopore/david_eccles_bc_ideas/mouse_ref.fa\",\n",
    "    \"hmm_params\": \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/david_hmm_params.pickle\",\n",
    "    \"ncores\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HMM_PARAMS = pickle.load(open(args[\"hmm_params\"], 'rb'))\n",
    "HMM_PARAMS = sorted(zip(*[HMM_PARAMS[\"kmers\"], HMM_PARAMS[\"means\"], HMM_PARAMS[\"stdv\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_transmat(nmers):\n",
    "    \"\"\"make a transition matrix assuming move=1\"\"\"\n",
    "    all_kmers = [x[0] for x in HMM_PARAMS]\n",
    "    n_components = len(all_kmers)\n",
    "    transmat = np.empty((n_components, n_components))\n",
    "    for j, from_kmer in enumerate(all_kmers):\n",
    "        for i, to_kmer in enumerate(all_kmers):\n",
    "            p = 1/4. if from_kmer[-(NMERS-1):] == to_kmer[:(NMERS-1)] else 0.\n",
    "            transmat[j, i] = p          \n",
    "            \n",
    "    return transmat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_transmat0(nmers):\n",
    "    \"\"\"make a transition matrix assuming move=0 or move=1\"\"\"\n",
    "    all_kmers = [x[0] for x in HMM_PARAMS]\n",
    "    n_components = len(all_kmers)\n",
    "    transmat = np.empty((n_components, n_components))\n",
    "    for j, from_kmer in enumerate(all_kmers):\n",
    "        for i, to_kmer in enumerate(all_kmers):\n",
    "            p = 0\n",
    "            if from_kmer[-(NMERS-1):] == to_kmer[:(NMERS-1)]:\n",
    "                \"\"\"move=1\"\"\"\n",
    "                p = (9/10.) * (1/4.) \n",
    "            elif from_kmer == to_kmer:\n",
    "                \"\"\"move=0\"\"\"\n",
    "                p = (1/10.) * 1\n",
    "            transmat[j, i] = p          \n",
    "            \n",
    "    return transmat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_transmat2(nmers):\n",
    "    \"\"\"make a transition matrix assuming move=0 or move=1 or move=2\"\"\"\n",
    "    all_kmers = [x[0] for x in HMM_PARAMS]\n",
    "    n_components = len(all_kmers)\n",
    "    transmat = np.empty((n_components, n_components))\n",
    "    for j, from_kmer in enumerate(all_kmers):\n",
    "        for i, to_kmer in enumerate(all_kmers):\n",
    "            p = 0\n",
    "            if from_kmer[-(NMERS-2):] == to_kmer[:(NMERS-2)]:\n",
    "                \"\"\"move=2\"\"\"\n",
    "                p = (3/10.) * (1/16.)\n",
    "            elif from_kmer[-(NMERS-1):] == to_kmer[:(NMERS-1)]:\n",
    "                \"\"\"move=1\"\"\"\n",
    "                p = (7/10.) * (1/4.) \n",
    "            elif from_kmer == to_kmer:\n",
    "                \"\"\"move=0\"\"\"\n",
    "                p = (0/10.) * 1\n",
    "            transmat[j, i] = p          \n",
    "            \n",
    "    return transmat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example code for a continuous HMM with gaussian emissions\n",
    "\n",
    "\n",
    "F = ghmm.Float()  # emission domain of this model\n",
    "\n",
    "A = mk_transmat2(NMERS)\n",
    "B = [[x[1], x[2]] for x in HMM_PARAMS]   # parameters of emission distributions in pairs of (mu, sigma)\n",
    "pi = [1/float(NSTATES)] * NSTATES   # initial probabilities per state\n",
    "\n",
    "# generate model from parameters\n",
    "model = ghmm.HMMFromMatrices(F,ghmm.GaussianDistribution(F), A, B, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianEmissionHMM(N=64)\n",
      "  state 0 (initial=0.02, mu=70.17, sigma=3.84)\n",
      "    Transitions: ->0 (0.02), ->1 (0.02), ->2 (0.02), ->3 (0.02), ->4 (0.02), ->5 (0.02), ->6 (0.02), ->7 (0.02), ->8 (0.02), ->9 (0.02), ->10 (0.02), ->11 (0.02), ->12 (0.02), ->13 (0.02), ->14 (0.02), ->15 (0.02)\n",
      "  state 1 (initial=0.02, mu=63.14, sigma=3.47)\n",
      "    Transitions: ->4 (0.17), ->5 (0.17), ->6 (0.17), ->7 (0.17), ->16 (0.02), ->17 (0.02), ->18 (0.02), ->19 (0.02), ->20 (0.02), ->21 (0.02), ->22 (0.02), ->23 (0.02), ->24 (0.02), ->25 (0.02), ->26 (0.02), ->27 (0.02), ->28 (0.02), ->29 (0.02), ->30 (0.02), ->31 (0.02)\n",
      "\n",
      "  ...\n",
      "\n",
      "  state 62 (initial=0.02, mu=53.52, sigma=2.85)\n",
      "    Transitions: ->32 (0.02), ->33 (0.02), ->34 (0.02), ->35 (0.02), ->36 (0.02), ->37 (0.02), ->38 (0.02), ->39 (0.02), ->40 (0.02), ->41 (0.02), ->42 (0.02), ->43 (0.02), ->44 (0.02), ->45 (0.02), ->46 (0.02), ->47 (0.02), ->56 (0.17), ->57 (0.17), ->58 (0.17), ->59 (0.17)\n",
      "  state 63 (initial=0.02, mu=49.86, sigma=2.78)\n",
      "    Transitions: ->48 (0.02), ->49 (0.02), ->50 (0.02), ->51 (0.02), ->52 (0.02), ->53 (0.02), ->54 (0.02), ->55 (0.02), ->56 (0.02), ->57 (0.02), ->58 (0.02), ->59 (0.02), ->60 (0.02), ->61 (0.02), ->62 (0.02), ->63 (0.02)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = str(model)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def result_to_seq(result):\n",
    "    states = result[0]\n",
    "    all_kmers = [x[0] for x in HMM_PARAMS]\n",
    "    kmers = [all_kmers[x] for x in states]\n",
    "    seq = [kmer[NMERS/2] for kmer in kmers]\n",
    "    return \"\".join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(means):\n",
    "    seq = ghmm.EmissionSequence(F, means)\n",
    "    result = model.viterbi(seq)\n",
    "    return result_to_seq(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'TTTGTGTGT'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([44.2, 44.3, 56, 58.2, 56.2, 58.1, 58.2, 60, 30.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert os.path.isfile(args[\"events\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>mmusMT_PCR1']\n",
      "GTTAATGTAGCTTAATAACAAAGCAAAGCACTGAAAATGCTTAGATGGATAATTGTATCCCATAAACACAAAGGTTTGGTCCTGGCCTTATAATTAATTA\n"
     ]
    }
   ],
   "source": [
    "ref_file = args[\"ref\"]\n",
    "test = !cat {ref_file} | grep \">\"\n",
    "print(test)\n",
    "ref = !cat {ref_file} | grep -v \">\"\n",
    "ref = ref[0]\n",
    "print(ref[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_data = pickle.load(open(args[\"events\"], 'rb'))\n",
    "file_data = [f for f in file_data if f is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepare_filemap(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fastq_file = \"{0}.fastq\".format(args[\"out_basename\"])\n",
    "mk_fastq(fastq_file, file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Index 14:48:41] Running in fast and sensitive mode. Two indexes will be used (double memory consumption).\n",
      "[Index 14:48:41] Index already exists. Loading from file.\n",
      "[Index 14:48:43] Secondary index already exists. Loading from file.\n",
      "[Index 14:48:45] Index loaded in 0.57 sec.\n",
      "[Index 14:48:45] Memory consumption: [currentRSS = 513 MB, peakRSS = 513 MB]\n",
      "\n",
      "[Run 14:48:45] Automatically setting the maximum allowed number of regions: max. 500, attempt to reduce after 100\n",
      "[Run 14:48:45] Reference genome is assumed to be linear.\n",
      "[Run 14:48:45] Only one alignment will be reported per mapped read.\n",
      "[ProcessReads 14:48:45] Reads will be loaded in batches of up to 200 MB in size.\n",
      "[ProcessReads 14:48:45] Batch of 81 reads (0 MiB) loaded in 0.00 sec. (7746680 bases)\n",
      "[ProcessReads 14:48:45] Memory consumption: [currentRSS = 514 MB, peakRSS = 514 MB]\n",
      "[ProcessReads 14:48:45] Using 4 threads.\n",
      "[ProcessReads 14:48:46] [CPU time: 3.46 sec, RSS: 549 MB] Read: 81/81 (100.00%) [m: 80, u: 1]                                                                      \n",
      "\n",
      "[ProcessReads 14:48:46] Memory consumption: [currentRSS = 549 MB, peakRSS = 582 MB]\n",
      "\n",
      "[ProcessReads 14:48:46] All reads processed in 3.46 sec (or 0.06 CPU min).\n"
     ]
    }
   ],
   "source": [
    "sam_file = \"{0}.sam\".format(args[\"out_basename\"])\n",
    "graphmap(ref_file, fastq_file, sam_file, args[\"ncores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[samopen] SAM header is present: 1 sequences.\r\n"
     ]
    }
   ],
   "source": [
    "bam_file = prepare_sam(args[\"out_basename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samfile = pysam.AlignmentFile(bam_file)\n",
    "reads  = [x for x in samfile.fetch()]\n",
    "len(reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basecall_read(params):\n",
    "    read_name, ref_pos = params\n",
    "    file_id, channel_id = get_file_and_channel(read_name)\n",
    "    print(file_id, channel_id, read_name)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    file_obj = get_file(channel_id, file_id)\n",
    "      \n",
    "    events = [x[\"mean\"] for x in file_obj[\"events\"].to_dict(\"records\")]\n",
    "    metrichor_seq = file_obj[\"fastq\"].split(\"\\n\")[1]\n",
    "    ref_seq = \"\".join([ref[x] for x in ref_pos])\n",
    "    called_seq = predict(events)\n",
    "    \n",
    "    stats = {\n",
    "        \"d_metrichor\": int(editdistance.eval(ref_seq, metrichor_seq)),\n",
    "        \"d_caller\": int(editdistance.eval(ref_seq, called_seq)), \n",
    "        \"length\": len(ref_seq),\n",
    "    }\n",
    "\n",
    "    return (read_name, called_seq, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 132, 'ch132_file10_read')\n",
      "(17, 132, 'ch132_file17_read')\n",
      "(16, 215, 'ch215_file16_read')\n",
      "(9, 215, 'ch215_file9_read')\n",
      "(35, 132, 'ch132_file35_read')\n",
      "(20, 211, 'ch211_file20_read')\n",
      "(12, 206, 'ch206_file12_read')\n",
      "(1, 203, 'ch203_file1_read')\n",
      "(11, 133, 'ch133_file11_read')\n",
      "(25, 215, 'ch215_file25_read')\n",
      "(11, 156, 'ch156_file11_read')\n",
      "(28, 211, 'ch211_file28_read')\n",
      "(1, 157, 'ch157_file1_read')\n",
      "(5, 227, 'ch227_file5_read')\n",
      "(10, 156, 'ch156_file10_read')\n",
      "(0, 135, 'ch135_file0_read')\n",
      "(12, 211, 'ch211_file12_read')\n",
      "(3, 211, 'ch211_file3_read')\n",
      "(19, 135, 'ch135_file19_read')\n",
      "(21, 212, 'ch212_file21_read')\n",
      "(12, 141, 'ch141_file12_read')\n",
      "(14, 215, 'ch215_file14_read')\n",
      "(18, 142, 'ch142_file18_read')\n",
      "(8, 141, 'ch141_file8_read')\n",
      "(3, 195, 'ch195_file3_read')\n",
      "(13, 141, 'ch141_file13_read')\n",
      "(15, 215, 'ch215_file15_read')\n",
      "(10, 215, 'ch215_file10_read')\n",
      "(24, 212, 'ch212_file24_read')\n",
      "(6, 156, 'ch156_file6_read')\n",
      "(27, 212, 'ch212_file27_read')\n",
      "(3, 156, 'ch156_file3_read')\n",
      "(0, 157, 'ch157_file0_read')\n",
      "(2, 203, 'ch203_file2_read')\n",
      "(26, 132, 'ch132_file26_read')\n",
      "(5, 204, 'ch204_file5_read')\n",
      "(17, 135, 'ch135_file17_read')\n",
      "(38, 132, 'ch132_file38_read')\n",
      "(11, 223, 'ch223_file11_read')\n",
      "(27, 215, 'ch215_file27_read')\n",
      "(6, 223, 'ch223_file6_read')\n",
      "(10, 206, 'ch206_file10_read')\n",
      "(14, 149, 'ch149_file14_read')\n",
      "(17, 215, 'ch215_file17_read')\n",
      "(44, 132, 'ch132_file44_read')\n",
      "(29, 211, 'ch211_file29_read')\n",
      "(7, 206, 'ch206_file7_read')\n",
      "(23, 156, 'ch156_file23_read')\n",
      "(8, 204, 'ch204_file8_read')\n",
      "(1, 201, 'ch201_file1_read')\n",
      "(14, 209, 'ch209_file14_read')\n",
      "(3, 142, 'ch142_file3_read')\n",
      "(21, 223, 'ch223_file21_read')\n",
      "(34, 206, 'ch206_file34_read')\n",
      "(32, 135, 'ch135_file32_read')\n",
      "(36, 132, 'ch132_file36_read')\n",
      "(17, 211, 'ch211_file17_read')\n",
      "(20, 206, 'ch206_file20_read')\n",
      "(33, 135, 'ch135_file33_read')\n",
      "(12, 149, 'ch149_file12_read')\n",
      "(4, 204, 'ch204_file4_read')\n",
      "(26, 211, 'ch211_file26_read')\n",
      "(23, 215, 'ch215_file23_read')\n",
      "(25, 135, 'ch135_file25_read')\n",
      "(25, 132, 'ch132_file25_read')\n",
      "(3, 149, 'ch149_file3_read')\n",
      "(2, 133, 'ch133_file2_read')\n",
      "(12, 215, 'ch215_file12_read')\n",
      "(15, 141, 'ch141_file15_read')\n",
      "(4, 223, 'ch223_file4_read')\n",
      "(23, 132, 'ch132_file23_read')\n",
      "(3, 132, 'ch132_file3_read')\n",
      "(3, 213, 'ch213_file3_read')\n",
      "(4, 215, 'ch215_file4_read')\n",
      "(9, 206, 'ch206_file9_read')\n",
      "(35, 211, 'ch211_file35_read')\n",
      "(15, 156, 'ch156_file15_read')\n",
      "(23, 135, 'ch135_file23_read')\n",
      "(21, 206, 'ch206_file21_read')\n",
      "(4, 206, 'ch206_file4_read')\n"
     ]
    }
   ],
   "source": [
    "p = Pool(args[\"ncores\"])\n",
    "## one can not access a read in parallel (deadlock for whatever reason)\n",
    "## therfore prepare input parameters outside of map\n",
    "input_params = [(read.query_name, read.get_reference_positions()) for read in reads]\n",
    "try:\n",
    "    results = p.map(basecall_read, input_params)\n",
    "except KeyboardInterrupt:\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers, seqs, stats = zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats = pandas.DataFrame(list(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats = stats.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_caller       222110\n",
       "d_metrichor    172867\n",
       "length         387166\n",
       "dtype: int64"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Performance: 77.829%\n"
     ]
    }
   ],
   "source": [
    "print(\"Relative Performance: {0:5.3f}%\".format(stats[\"d_metrichor\"] * 100 /float(stats[\"d_caller\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fasta_file_called = \"{0}.called.fa\".format(args[\"out_basename\"])\n",
    "with open(fasta_file_called, 'w') as f: \n",
    "    for header, seq in zip(headers, seqs): \n",
    "        f.write(\">\" + header + \"\\n\")\n",
    "        f.write(seq + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Index 14:49:17] Running in fast and sensitive mode. Two indexes will be used (double memory consumption).\n",
      "[Index 14:49:17] Index already exists. Loading from file.\n",
      "[Index 14:49:18] Secondary index already exists. Loading from file.\n",
      "[Index 14:49:20] Index loaded in 0.55 sec.\n",
      "[Index 14:49:20] Memory consumption: [currentRSS = 513 MB, peakRSS = 513 MB]\n",
      "\n",
      "[Run 14:49:20] Automatically setting the maximum allowed number of regions: max. 500, attempt to reduce after 100\n",
      "[Run 14:49:20] Reference genome is assumed to be linear.\n",
      "[Run 14:49:20] Only one alignment will be reported per mapped read.\n",
      "[ProcessReads 14:49:20] Reads will be loaded in batches of up to 200 MB in size.\n",
      "[ProcessReads 14:49:20] Batch of 80 reads (0 MiB) loaded in 0.00 sec. (13427848 bases)\n",
      "[ProcessReads 14:49:20] Memory consumption: [currentRSS = 513 MB, peakRSS = 513 MB]\n",
      "[ProcessReads 14:49:20] Using 4 threads.\n",
      "[ProcessReads 14:49:22] [CPU time: 3.20 sec, RSS: 531 MB] Read: 80/80 (100.00%) [m: 67, u: 13]                                                                     \n",
      "\n",
      "[ProcessReads 14:49:22] Memory consumption: [currentRSS = 531 MB, peakRSS = 584 MB]\n",
      "\n",
      "[ProcessReads 14:49:22] All reads processed in 3.20 sec (or 0.05 CPU min).\n"
     ]
    }
   ],
   "source": [
    "sam_file_called = \"{0}.called.sam\".format(args[\"out_basename\"])\n",
    "graphmap(ref_file, fasta_file_called, sam_file_called, args[\"ncores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[samopen] SAM header is present: 1 sequences.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/david_calling.called.sorted.bam'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sam(\"{0}.called\".format(args[\"out_basename\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 2)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
