{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ghmm\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import pysam\n",
    "import os\n",
    "import pandas\n",
    "import re\n",
    "import editdistance\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "%run \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/alignment_lib.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NMERS = 5\n",
    "NSTATES = 4**NMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# args = {\n",
    "#     \"events\" : \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/david_events.template.pickle\",\n",
    "#     \"out_basename\" : \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/david_calling\",\n",
    "#     \"ref\": \"/home/ibis/gregor.sturm/nanopore/david_eccles_bc_ideas/mouse_ref.fa\",\n",
    "#     \"hmm_params\": \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/david_hmm_params.pickle\",\n",
    "#     \"ncores\": 4\n",
    "# }\n",
    "\n",
    "args = {\n",
    "    \"events\" : \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/lambda_events.template.pickle\",\n",
    "    \"out_basename\" : \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/lambda_calling\",\n",
    "    \"ref\": \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/lambda_ref.fasta\",\n",
    "    \"hmm_params\": \"/home/ibis/gregor.sturm/nanopore/own/notebooks/03_pipeline/lambda_hmm_params.pickle\",\n",
    "    \"ncores\": 62\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HMM_PARAMS = pickle.load(open(args[\"hmm_params\"], 'rb'))\n",
    "HMM_PARAMS = sorted(zip(*[HMM_PARAMS[\"kmers\"], HMM_PARAMS[\"means\"], HMM_PARAMS[\"stdv\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_transmat(nmers):\n",
    "    \"\"\"make a transition matrix assuming move=1\"\"\"\n",
    "    all_kmers = [x[0] for x in HMM_PARAMS]\n",
    "    n_components = len(all_kmers)\n",
    "    transmat = np.empty((n_components, n_components))\n",
    "    for j, from_kmer in enumerate(all_kmers):\n",
    "        for i, to_kmer in enumerate(all_kmers):\n",
    "            p = 1/4. if from_kmer[-(NMERS-1):] == to_kmer[:(NMERS-1)] else 0.\n",
    "            transmat[j, i] = p          \n",
    "            \n",
    "    return transmat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_transmat0(nmers):\n",
    "    \"\"\"make a transition matrix assuming move=0 or move=1\"\"\"\n",
    "    all_kmers = [x[0] for x in HMM_PARAMS]\n",
    "    n_components = len(all_kmers)\n",
    "    transmat = np.empty((n_components, n_components))\n",
    "    for j, from_kmer in enumerate(all_kmers):\n",
    "        for i, to_kmer in enumerate(all_kmers):\n",
    "            p = 0\n",
    "            if from_kmer[-(NMERS-1):] == to_kmer[:(NMERS-1)]:\n",
    "                \"\"\"move=1\"\"\"\n",
    "                p = (9/10.) * (1/4.) \n",
    "            elif from_kmer == to_kmer:\n",
    "                \"\"\"move=0\"\"\"\n",
    "                p = (1/10.) * 1\n",
    "            transmat[j, i] = p          \n",
    "            \n",
    "    return transmat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_transmat2(nmers):\n",
    "    \"\"\"make a transition matrix assuming move=0 or move=1 or move=2\"\"\"\n",
    "    all_kmers = [x[0] for x in HMM_PARAMS]\n",
    "    n_components = len(all_kmers)\n",
    "    transmat = np.empty((n_components, n_components))\n",
    "    for j, from_kmer in enumerate(all_kmers):\n",
    "        for i, to_kmer in enumerate(all_kmers):\n",
    "            p = 0\n",
    "            if from_kmer[-(NMERS-2):] == to_kmer[:(NMERS-2)]:\n",
    "                \"\"\"move=2\"\"\"\n",
    "                p = (3/10.) * (1/16.)\n",
    "            elif from_kmer[-(NMERS-1):] == to_kmer[:(NMERS-1)]:\n",
    "                \"\"\"move=1\"\"\"\n",
    "                p = (7/10.) * (1/4.) \n",
    "            elif from_kmer == to_kmer:\n",
    "                \"\"\"move=0\"\"\"\n",
    "                p = (0/10.) * 1\n",
    "            transmat[j, i] = p          \n",
    "            \n",
    "    return transmat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example code for a continuous HMM with gaussian emissions\n",
    "\n",
    "\n",
    "F = ghmm.Float()  # emission domain of this model\n",
    "\n",
    "A = mk_transmat2(NMERS)\n",
    "B = [[x[1], x[2]] for x in HMM_PARAMS]   # parameters of emission distributions in pairs of (mu, sigma)\n",
    "pi = [1/float(NSTATES)] * NSTATES   # initial probabilities per state\n",
    "\n",
    "# generate model from parameters\n",
    "model = ghmm.HMMFromMatrices(F,ghmm.GaussianDistribution(F), A, B, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianEmissionHMM(N=64)\n",
      "  state 0 (initial=0.00, mu=67.75, sigma=3.49)\n",
      "    Transitions: ->0 (0.02)\n",
      "  state 1 (initial=0.00, mu=61.09, sigma=3.02)\n",
      "    Transitions: ->1 (0.02)\n",
      "\n",
      "  ...\n",
      "\n",
      "  state 62 (initial=0.00, mu=52.22, sigma=2.39)\n",
      "    Transitions: ->62 (0.02)\n",
      "  state 63 (initial=0.00, mu=47.64, sigma=2.26)\n",
      "    Transitions: ->63 (0.02)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = str(model)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def result_to_seq(result):\n",
    "    states = result[0]\n",
    "    all_kmers = [x[0] for x in HMM_PARAMS]\n",
    "    kmers = [all_kmers[x] for x in states]\n",
    "    seq = [kmer[NMERS/2] for kmer in kmers]\n",
    "    return \"\".join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(means):\n",
    "    seq = ghmm.EmissionSequence(F, means)\n",
    "    result = model.viterbi(seq)\n",
    "    return result_to_seq(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'TTTTTTTTT'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([44.2, 44.3, 56, 58.2, 56.2, 58.1, 58.2, 60, 30.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ibis/gregor.sturm/nanopore/own/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert os.path.isfile(args[\"events\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>burn-in lambda_ref']\n",
      "GGGCGGCGACCTCGCGGGTTTTCGCTATTTATGAAAATTTTCCGGTTTAAGGCGTTTCCGTTCTTCTTCGTCATAACTTAATGTTTTTATTTAAAATACC\n"
     ]
    }
   ],
   "source": [
    "ref_file = args[\"ref\"]\n",
    "test = !cat {ref_file} | grep \">\"\n",
    "print(test)\n",
    "ref = !cat {ref_file} | grep -v \">\"\n",
    "ref = ref[0]\n",
    "print(ref[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_data = pickle.load(open(args[\"events\"], 'rb'))\n",
    "file_data = [f for f in file_data if f is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepare_filemap(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fastq_file = \"{0}.fastq\".format(args[\"out_basename\"])\n",
    "mk_fastq(fastq_file, file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Index 17:26:03] Running in fast and sensitive mode. Two indexes will be used (double memory consumption).\n",
      "[Index 17:26:03] Index already exists. Loading from file.\n",
      "[Index 17:26:03] Secondary index already exists. Loading from file.\n",
      "[Index 17:26:04] Index loaded in 0.46 sec.\n",
      "[Index 17:26:04] Memory consumption: [currentRSS = 516 MB, peakRSS = 21837 MB]\n",
      "\n",
      "[Run 17:26:04] Automatically setting the maximum allowed number of regions: max. 500, attempt to reduce after 100\n",
      "[Run 17:26:04] Reference genome is assumed to be linear.\n",
      "[Run 17:26:04] Only one alignment will be reported per mapped read.\n",
      "[ProcessReads 17:26:04] Reads will be loaded in batches of up to 200 MB in size.\n",
      "[ProcessReads 17:26:04] Batch of 6123 reads (64 MiB) loaded in 0.47 sec. (37348520 bases)\n",
      "[ProcessReads 17:26:04] Memory consumption: [currentRSS = 582 MB, peakRSS = 21837 MB]\n",
      "[ProcessReads 17:26:04] Using 62 threads.\n",
      "[ProcessReads 17:26:59] [CPU time: 1942.49 sec, RSS: 647 MB] Read: 6123/6123 (100.00%) [m: 5709, u: 414]                                                           \n",
      "\n",
      "[ProcessReads 17:26:59] Memory consumption: [currentRSS = 646 MB, peakRSS = 21837 MB]\n",
      "\n",
      "[ProcessReads 17:26:59] All reads processed in 1942.53 sec (or 32.38 CPU min).\n"
     ]
    }
   ],
   "source": [
    "sam_file = \"{0}.sam\".format(args[\"out_basename\"])\n",
    "graphmap(ref_file, fastq_file, sam_file, args[\"ncores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bam_file = prepare_sam(args[\"out_basename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5709"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samfile = pysam.AlignmentFile(bam_file)\n",
    "reads  = [x for x in samfile.fetch()]\n",
    "len(reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basecall_read(params):\n",
    "    read_name, ref_pos = params\n",
    "    file_id, channel_id = get_file_and_channel(read_name)\n",
    "    print(file_id, channel_id, read_name)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    file_obj = get_file(channel_id, file_id)\n",
    "      \n",
    "    events = [x[\"mean\"] for x in file_obj[\"events\"].to_dict(\"records\")]\n",
    "    metrichor_seq = file_obj[\"fastq\"].split(\"\\n\")[1]\n",
    "    ref_seq = \"\".join([ref[x] for x in ref_pos])\n",
    "    called_seq = predict(events)\n",
    "    \n",
    "    stats = {\n",
    "        \"d_metrichor\": int(editdistance.eval(ref_seq, metrichor_seq)),\n",
    "        \"d_caller\": int(editdistance.eval(ref_seq, called_seq)), \n",
    "        \"length\": len(ref_seq),\n",
    "    }\n",
    "\n",
    "    return (read_name, called_seq, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = Pool(args[\"ncores\"])\n",
    "## one can not access a read in parallel (deadlock for whatever reason)\n",
    "## therfore prepare input parameters outside of map\n",
    "input_params = [(read.query_name, read.get_reference_positions()) for read in reads]\n",
    "try:\n",
    "    results = p.map(basecall_read, input_params)\n",
    "except KeyboardInterrupt:\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers, seqs, stats = zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats = pandas.DataFrame(list(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats = stats.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Relative Performance: {0:5.3f}%\".format(stats[\"d_metrichor\"] * 100 /float(stats[\"d_caller\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fasta_file_called = \"{0}.called.fa\".format(args[\"out_basename\"])\n",
    "with open(fasta_file_called, 'w') as f: \n",
    "    for header, seq in zip(headers, seqs): \n",
    "        f.write(\">\" + header + \"\\n\")\n",
    "        f.write(seq + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sam_file_called = \"{0}.called.sam\".format(args[\"out_basename\"])\n",
    "graphmap(ref_file, fasta_file_called, sam_file_called, args[\"ncores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_sam(\"{0}.called\".format(args[\"out_basename\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH einser ipython2",
   "language": "",
   "name": "rik_ssh_einser_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
