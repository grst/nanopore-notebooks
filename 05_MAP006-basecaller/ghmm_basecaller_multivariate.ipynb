{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ghmm\n",
    "from collections import OrderedDict\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "from itertools import product as iterproduct, chain\n",
    "from pprint import pprint\n",
    "import pysam\n",
    "import os\n",
    "import pandas\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import editdistance\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "from nbwrapper import getargs\n",
    "from multiprocessing import Pool\n",
    "%run \"/home/ibis/gregor.sturm/nanopore/own/notebooks/05_MAP006-basecaller/lib/alignment_validation.ipynb\"\n",
    "%run \"/home/ibis/gregor.sturm/nanopore/own/notebooks/05_MAP006-basecaller/lib/alignment_lib.ipynb\"\n",
    "%run \"/home/ibis/gregor.sturm/nanopore/own/notebooks/05_MAP006-basecaller/lib/analysis_lib.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = getargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "breakpoint": false
    }
   },
   "outputs": [],
   "source": [
    "NMERS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"events\" : \"/home/ibis/gregor.sturm/nanopore/own/notebooks/05_MAP006-basecaller/loman006-1_100.events.template.pickle\",\n",
    "    \"out_basename\" : \"/home/ibis/gregor.sturm/nanopore/own/notebooks/05_MAP006-basecaller/loman006-1_100.called\",\n",
    "    \"ref\": \"/home/ibis/gregor.sturm/nanopore/NanoporeData/PublicData/LomanLab_MAP-006/ecoli_mg1655.fa\",\n",
    "    \"hmm_params\": \"/home/ibis/gregor.sturm/nanopore/own/notebooks/05_MAP006-basecaller/loman006-1.model.computed.pickle\",\n",
    "    \"ncores\": 62,\n",
    "    \"nmers\": NMERS,\n",
    "    \"multivariate\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NMERS = int(args[\"nmers\"])\n",
    "NSTATES = 4**NMERS\n",
    "MULTIVARIATE = bool(int(args[\"multivariate\"]))\n",
    "args[\"ncores\"] = int(args[\"ncores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HMM_PARAMS = pickle.load(open(args[\"hmm_params\"], 'rb'))\n",
    "HMM_PARAMS = HMM_PARAMS[\"/opt/chimaera/model/r7.3_e6_70bps_6mer/template_median68pA.model\"]\n",
    "ALL_KMERS = [\"\".join(x) for x in iterproduct(\"ACGT\", repeat=NMERS)]\n",
    "assert HMM_PARAMS[\"kmer\"].tolist() == ALL_KMERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_transmat1(nmers):\n",
    "    \"\"\"make a transition matrix assuming move=1\"\"\"\n",
    "    n_components = len(ALL_KMERS)\n",
    "    transmat = np.empty((n_components, n_components))\n",
    "    for j, from_kmer in enumerate(ALL_KMERS):\n",
    "        for i, to_kmer in enumerate(ALL_KMERS):\n",
    "            p = 1/4. if from_kmer[-(NMERS-1):] == to_kmer[:(NMERS-1)] else 0.\n",
    "            transmat[j, i] = p          \n",
    "            \n",
    "    return transmat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_transmat0(nmers):\n",
    "    \"\"\"make a transition matrix assuming move=0 or move=1\"\"\"\n",
    "    n_components = len(ALL_KMERS)\n",
    "    transmat = np.empty((n_components, n_components))\n",
    "    for j, from_kmer in enumerate(ALL_KMERS):\n",
    "        for i, to_kmer in enumerate(ALL_KMERS):\n",
    "            p = 0\n",
    "            if from_kmer[-(NMERS-1):] == to_kmer[:(NMERS-1)]:\n",
    "                \"\"\"move=1\"\"\"\n",
    "                p = (9/10.) * (1/4.) \n",
    "            elif from_kmer == to_kmer:\n",
    "                \"\"\"move=0\"\"\"\n",
    "                p = (1/10.) * 1\n",
    "            transmat[j, i] = p          \n",
    "            \n",
    "    return transmat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_transmat2(nmers):\n",
    "    \"\"\"make a transition matrix assuming move=0 or move=1 or move=2\"\"\"\n",
    "    n_components = len(ALL_KMERS)\n",
    "    transmat = np.empty((n_components, n_components))\n",
    "    for j, from_kmer in enumerate(ALL_KMERS):\n",
    "        for i, to_kmer in enumerate(ALL_KMERS):\n",
    "            p = 0\n",
    "            if from_kmer[-(NMERS-2):] == to_kmer[:(NMERS-2)]:\n",
    "                \"\"\"move=2\"\"\"\n",
    "                p = (2/50.) * (1/16.)\n",
    "            elif from_kmer[-(NMERS-1):] == to_kmer[:(NMERS-1)]:\n",
    "                \"\"\"move=1\"\"\"\n",
    "                p = (47/50.) * (1/4.) \n",
    "            elif from_kmer == to_kmer:\n",
    "                \"\"\"move=0\"\"\"\n",
    "                p = (1/50.) * 1\n",
    "            transmat[j, i] = p          \n",
    "            \n",
    "    return transmat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mk_transmat = mk_transmat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mk_model_simple(): \n",
    "    \"\"\" simple model, only taking the means into account. \"\"\"\n",
    "    A = mk_transmat(NMERS)\n",
    "    B = HMM_PARAMS[[\"level_mean\", \"level_stdv\"]].values.tolist() #mu, std of each state\n",
    "    pi = [1/float(NSTATES)] * NSTATES   # initial probabilities per state\n",
    "    # generate model from parameters\n",
    "    model = ghmm.HMMFromMatrices(F,ghmm.GaussianDistribution(F), A, B, pi)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = ghmm.Float()  # emission domain of this model\n",
    "def mk_model():\n",
    "    if MULTIVARIATE: \n",
    "        return mk_model_multivariate()\n",
    "    else: \n",
    "        return mk_model_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianEmissionHMM(N=4096)\n",
      "  state 0 (initial=0.00, mu=62.96, sigma=0.85)\n",
      "    Transitions: ->0 (0.00), ->1 (0.00), ->2 (0.00), ->3 (0.00), ->4 (0.00), ->5 (0.00), ->6 (0.00), ->7 (0.00), ->8 (0.00), ->9 (0.00), ->10 (0.00), ->11 (0.00), ->12 (0.00), ->13 (0.00), ->14 (0.00), ->15 (0.00)\n",
      "  state 1 (initial=0.00, mu=58.16, sigma=0.68)\n",
      "    Transitions: ->1 (0.02), ->4 (0.23), ->5 (0.23), ->6 (0.23), ->7 (0.23), ->16 (0.00), ->17 (0.00), ->18 (0.00), ->19 (0.00), ->20 (0.00), ->21 (0.00), ->22 (0.00), ->23 (0.00), ->24 (0.00), ->25 (0.00), ->26 (0.00), ->27 (0.00), ->28 (0.00), ->29 (0.00), ->30 (0.00), ->31 (0.00)\n",
      "\n",
      "  ...\n",
      "\n",
      "  state 4094 (initial=0.00, mu=46.66, sigma=0.83)\n",
      "    Transitions: ->4064 (0.00), ->4065 (0.00), ->4066 (0.00), ->4067 (0.00), ->4068 (0.00), ->4069 (0.00), ->4070 (0.00), ->4071 (0.00), ->4072 (0.00), ->4073 (0.00), ->4074 (0.00), ->4075 (0.00), ->4076 (0.00), ->4077 (0.00), ->4078 (0.00), ->4079 (0.00), ->4088 (0.23), ->4089 (0.23), ->4090 (0.23), ->4091 (0.23), ->4094 (0.02)\n",
      "  state 4095 (initial=0.00, mu=45.07, sigma=1.36)\n",
      "    Transitions: ->4080 (0.00), ->4081 (0.00), ->4082 (0.00), ->4083 (0.00), ->4084 (0.00), ->4085 (0.00), ->4086 (0.00), ->4087 (0.00), ->4088 (0.00), ->4089 (0.00), ->4090 (0.00), ->4091 (0.00), ->4092 (0.00), ->4093 (0.00), ->4094 (0.00), ->4095 (0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = mk_model()\n",
    "s = str(model)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def result_to_seq(result):\n",
    "    states = result[0]\n",
    "    kmers = [ALL_KMERS[x] for x in states]\n",
    "    seq = [kmer[0] for kmer in kmers] + [kmers[-1][1:]]\n",
    "    return \"\".join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(mixed):\n",
    "    \"\"\"mixed is a set of tuples (event_mean, event_stdv)\"\"\"\n",
    "    if MULTIVARIATE: \n",
    "        emissions = [x for x in chain(*mixed)]    \n",
    "    else: \n",
    "        emissions = [x[0] for x in mixed]\n",
    "    seq = ghmm.EmissionSequence(F, emissions)\n",
    "    result = model.viterbi(seq)\n",
    "    return result_to_seq(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = model.sampleSingle(10)\n",
    "s = [x for x in s]\n",
    "seq = zip([s[i] for i in range(0, len(s), 2)], [s[i] for i in range(1, len(s), 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TCGATGGATC'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ghmm.GaussianEmissionHMM at 0x7fa0f2212190>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ibis/gregor.sturm/nanopore/own/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert os.path.isfile(args[\"events\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>gi|556503834|ref|NC_000913.3| Escherichia coli str. K-12 substr. MG1655, complete genome']\n",
      "AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGCTTCTGAACTGGTTACCTGCCGTGAGTAAAT\n"
     ]
    }
   ],
   "source": [
    "ref = load_ref(args[\"ref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_data = pickle.load(open(args[\"events\"], 'rb'))\n",
    "file_data = [f for f in file_data if f is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_filemap(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_read(file_obj):    \n",
    "    events = [x[\"mean\"] for x in file_obj[\"events\"].to_dict(\"records\")]\n",
    "    seq = ghmm.EmissionSequence(F, events)\n",
    "    model.baumWelch(seq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basecall_read(file_obj):\n",
    "    file_obj = normalize_read(file_obj, target_median=59)\n",
    "    events = [(x[\"mean\"], x[\"stdv\"]) for x in file_obj[\"events\"].to_dict(\"records\")]\n",
    "    called_seq = predict(events)\n",
    "    return (file_obj[\"channel\"], file_obj[\"file_id\"], called_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"\"\" train with baum-welch \"\"\"\n",
    "# for i, file_obj in enumerate(file_data): \n",
    "#     sys.stdout.write('\\rdone {0:%}'.format(i/float(len(file_data))))\n",
    "#     train_read(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "breakpoint": false
    }
   },
   "outputs": [],
   "source": [
    "p = Pool(args[\"ncores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "done 100.000000%"
     ]
    }
   ],
   "source": [
    "print(\"Prediction: \")\n",
    "results = []\n",
    "try:\n",
    "    for i, res in enumerate(p.imap_unordered(basecall_read, file_data), 1):\n",
    "        results.append(res)\n",
    "        sys.stdout.write('\\rdone {0:%}'.format(i/float(len(file_data))))\n",
    "    p.close()\n",
    "    p.join()\n",
    "except KeyboardInterrupt:\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types = [\"metrichor\", \"called\", \"random\"]\n",
    "fasta_files = {t: \"{0}.{1}.fa\".format(args[\"out_basename\"], t) for t in types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## metrichor fasta\n",
    "with open(fasta_files[\"metrichor\"], 'w') as f: \n",
    "    for file_obj in file_data: \n",
    "        f.write(\">ch{0}_file{1}_metrichor\".format(file_obj[\"channel\"], file_obj[\"file_id\"])+ \"\\n\")\n",
    "        f.write(file_obj[\"fastq\"].split(\"\\n\")[1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## called fasta/random fasta\n",
    "with open(fasta_files[\"called\"], 'w') as f: \n",
    "    with open(fasta_files[\"random\"], 'w') as fr:\n",
    "        for channel, file_id, seq in results: \n",
    "            f.write(\">ch{0}_file{1}_called\".format(channel, file_id)+ \"\\n\")\n",
    "            fr.write(\">ch{0}_file{1}_random\".format(channel, file_id)+ \"\\n\")\n",
    "            f.write(seq + \"\\n\")\n",
    "            fr.write(\"\".join([random.choice(\"ACGT\") for _ in range(len(seq))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Index 17:59:28] Running in fast and sensitive mode. Two indexes will be used (double memory consumption).\n",
      "[Index 17:59:28] Index already exists. Loading from file.\n",
      "[Index 17:59:29] Secondary index already exists. Loading from file.\n",
      "[Index 17:59:30] Index loaded in 1.02 sec.\n",
      "[Index 17:59:30] Memory consumption: [currentRSS = 674 MB, peakRSS = 1113 MB]\n",
      "\n",
      "[Run 17:59:30] Automatically setting the maximum allowed number of regions: max. 500, attempt to reduce after 100\n",
      "[Run 17:59:30] Reference genome is assumed to be linear.\n",
      "[Run 17:59:30] Only one alignment will be reported per mapped read.\n",
      "[ProcessReads 17:59:30] Reads will be loaded in batches of up to 200 MB in size.\n",
      "[ProcessReads 17:59:30] Batch of 100 reads (0 MiB) loaded in 0.01 sec. (16786664 bases)\n",
      "[ProcessReads 17:59:30] Memory consumption: [currentRSS = 675 MB, peakRSS = 1113 MB]\n",
      "[ProcessReads 17:59:30] Using 62 threads.\n",
      "[ProcessReads 17:59:33] [CPU time: 40.88 sec, RSS: 698 MB] Read: 100/100 (100.00%) [m: 100, u: 0]                                                                  \n",
      "\n",
      "[ProcessReads 17:59:33] Memory consumption: [currentRSS = 698 MB, peakRSS = 1587 MB]\n",
      "\n",
      "[ProcessReads 17:59:33] All reads processed in 40.89 sec (or 0.68 CPU min).\n",
      "[Index 17:59:34] Running in fast and sensitive mode. Two indexes will be used (double memory consumption).\n",
      "[Index 17:59:34] Index already exists. Loading from file.\n",
      "[Index 17:59:35] Secondary index already exists. Loading from file.\n",
      "[Index 17:59:35] Index loaded in 0.71 sec.\n",
      "[Index 17:59:35] Memory consumption: [currentRSS = 674 MB, peakRSS = 1113 MB]\n",
      "\n",
      "[Run 17:59:35] Automatically setting the maximum allowed number of regions: max. 500, attempt to reduce after 100\n",
      "[Run 17:59:35] Reference genome is assumed to be linear.\n",
      "[Run 17:59:35] Only one alignment will be reported per mapped read.\n",
      "[ProcessReads 17:59:35] Reads will be loaded in batches of up to 200 MB in size.\n",
      "[ProcessReads 17:59:35] Batch of 100 reads (0 MiB) loaded in 0.01 sec. (18572520 bases)\n",
      "[ProcessReads 17:59:35] Memory consumption: [currentRSS = 675 MB, peakRSS = 1113 MB]\n",
      "[ProcessReads 17:59:35] Using 62 threads.\n",
      "[ProcessReads 17:59:39] [CPU time: 38.28 sec, RSS: 694 MB] Read: 100/100 (100.00%) [m: 99, u: 1]                                                                   \n",
      "\n",
      "[ProcessReads 17:59:39] Memory consumption: [currentRSS = 694 MB, peakRSS = 1434 MB]\n",
      "\n",
      "[ProcessReads 17:59:39] All reads processed in 38.29 sec (or 0.64 CPU min).\n",
      "[Index 17:59:40] Running in fast and sensitive mode. Two indexes will be used (double memory consumption).\n",
      "[Index 17:59:40] Index already exists. Loading from file.\n",
      "[Index 17:59:41] Secondary index already exists. Loading from file.\n",
      "[Index 17:59:41] Index loaded in 0.70 sec.\n",
      "[Index 17:59:41] Memory consumption: [currentRSS = 674 MB, peakRSS = 1113 MB]\n",
      "\n",
      "[Run 17:59:41] Automatically setting the maximum allowed number of regions: max. 500, attempt to reduce after 100\n",
      "[Run 17:59:41] Reference genome is assumed to be linear.\n",
      "[Run 17:59:41] Only one alignment will be reported per mapped read.\n",
      "[ProcessReads 17:59:41] Reads will be loaded in batches of up to 200 MB in size.\n",
      "[ProcessReads 17:59:41] Batch of 100 reads (0 MiB) loaded in 0.01 sec. (24683752 bases)\n",
      "[ProcessReads 17:59:41] Memory consumption: [currentRSS = 675 MB, peakRSS = 1113 MB]\n",
      "[ProcessReads 17:59:41] Using 62 threads.\n",
      "[ProcessReads 17:59:45] [CPU time: 47.70 sec, RSS: 690 MB] Read: 100/100 (100.00%) [m: 67, u: 33]                                                                  \n",
      "\n",
      "[ProcessReads 17:59:45] Memory consumption: [currentRSS = 690 MB, peakRSS = 1327 MB]\n",
      "\n",
      "[ProcessReads 17:59:45] All reads processed in 47.71 sec (or 0.80 CPU min).\n"
     ]
    }
   ],
   "source": [
    "for t in types: \n",
    "    sam_file = \"{0}.{1}.sam\".format(args[\"out_basename\"], t)\n",
    "    graphmap(args[\"ref\"], fasta_files[t], sam_file, args[\"ncores\"])\n",
    "    prepare_sam(\"{0}.{1}\".format(args[\"out_basename\"], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mk_stat(t):\n",
    "    samfile = \"{0}.{1}.sorted.bam\".format(args[\"out_basename\"], t)\n",
    "    sst = samstats(samfile, ref, ncores=args[\"ncores\"])\n",
    "    return pandas.DataFrame(sst.print_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metrichor', 'called', 'random']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='float: left;'><div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mapped_reads/total_reads</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000000%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>significant_reads/total_reads</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>99.000000%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mapped_nts/total_nts</td>\n",
       "      <td>865397</td>\n",
       "      <td>970301</td>\n",
       "      <td>89.188510%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>editdistance/alignment_length</td>\n",
       "      <td>311257</td>\n",
       "      <td>948375</td>\n",
       "      <td>32.820034%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alignment_score/alignment_length</td>\n",
       "      <td>1802584</td>\n",
       "      <td>948375</td>\n",
       "      <td>190.070805%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SNPs/mapped_nts</td>\n",
       "      <td>123375</td>\n",
       "      <td>865397</td>\n",
       "      <td>14.256463%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ins/mapped_nts</td>\n",
       "      <td>104806</td>\n",
       "      <td>865397</td>\n",
       "      <td>12.110742%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>del/mapped_nts</td>\n",
       "      <td>82978</td>\n",
       "      <td>865397</td>\n",
       "      <td>9.588432%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div><div style='float: left;'><div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mapped_reads/total_reads</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>99.000000%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>significant_reads/total_reads</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>95.000000%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mapped_nts/total_nts</td>\n",
       "      <td>828858</td>\n",
       "      <td>981932</td>\n",
       "      <td>84.410937%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>editdistance/alignment_length</td>\n",
       "      <td>386519</td>\n",
       "      <td>926345</td>\n",
       "      <td>41.725167%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alignment_score/alignment_length</td>\n",
       "      <td>1006518</td>\n",
       "      <td>926345</td>\n",
       "      <td>108.654767%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SNPs/mapped_nts</td>\n",
       "      <td>155782</td>\n",
       "      <td>828858</td>\n",
       "      <td>18.794775%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ins/mapped_nts</td>\n",
       "      <td>133175</td>\n",
       "      <td>828858</td>\n",
       "      <td>16.067288%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>del/mapped_nts</td>\n",
       "      <td>97487</td>\n",
       "      <td>828858</td>\n",
       "      <td>11.761605%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div><div style='float: left;'><div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mapped_reads/total_reads</td>\n",
       "      <td>67</td>\n",
       "      <td>100</td>\n",
       "      <td>67.000000%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>significant_reads/total_reads</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.000000%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mapped_nts/total_nts</td>\n",
       "      <td>529012</td>\n",
       "      <td>981932</td>\n",
       "      <td>53.874606%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>editdistance/alignment_length</td>\n",
       "      <td>338125</td>\n",
       "      <td>563313</td>\n",
       "      <td>60.024356%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alignment_score/alignment_length</td>\n",
       "      <td>-97991</td>\n",
       "      <td>563313</td>\n",
       "      <td>-17.395480%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SNPs/mapped_nts</td>\n",
       "      <td>146027</td>\n",
       "      <td>529012</td>\n",
       "      <td>27.603722%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ins/mapped_nts</td>\n",
       "      <td>157760</td>\n",
       "      <td>529012</td>\n",
       "      <td>29.821630%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>del/mapped_nts</td>\n",
       "      <td>34301</td>\n",
       "      <td>529012</td>\n",
       "      <td>6.483974%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>"
      ],
      "text/plain": [
       "<__main__.side_by_side instance at 0x7fa0ec339908>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = map(mk_stat, types)\n",
    "print(types)\n",
    "side_by_side(*stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for t, df in zip(types, stats):\n",
    "#     with open(\"{0}.stats.{1}.html\".format(args[\"out_basename\"], t), 'w') as f:\n",
    "#         f.write(df.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def score_consensus(t):\n",
    "#     consensus = mk_consensus(\"{0}.{1}.sorted.bam\".format(args[\"out_basename\"], t), ref_file)\n",
    "#     return(consensus)\n",
    "#     consensus = consensus.split(\"\\n\")[1].to_upper()\n",
    "#     score = needle(ref, consensus)\n",
    "#     return (consensus, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# p = Pool(args[\"ncores\"])\n",
    "# try:\n",
    "#     consensus = p.map(score_consensus, types)\n",
    "#     p.close()\n",
    "# except KeyboardInterrupt:\n",
    "#     p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mk_consensus(\"{0}.{1}.sorted.bam\".format(args[\"out_basename\"], \"metrichor\"), ref_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH einser ipython2",
   "language": "",
   "name": "rik_ssh_einser_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
